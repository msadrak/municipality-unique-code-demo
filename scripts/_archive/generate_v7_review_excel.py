"""
Config Generator V7 - Reviewable Excel with Smart Clustering
=============================================================
Generates a reviewable Excel file where users can manually fix activity titles.

Key Improvements:
1. Longest Match Priority: Prefers 4-word prefixes over 2-word ones
2. Connector Awareness: Extends prefixes ending in (Ùˆ, Ø¯Ø±, Ø¨Ù‡, Ø§Ø², Ø¨Ø§, Ø¨Ø±Ø§ÛŒ)
3. Raw Description Fallback: Shows raw text for manual review instead of "Ø³Ø§ÛŒØ±"

Usage:
    python scripts/generate_v7_review_excel.py
"""

import pandas as pd
import re
from collections import defaultdict, Counter
from typing import Optional, List, Dict, Tuple
from openpyxl import load_workbook
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill


# ============================================================
# CONFIGURATION
# ============================================================

INPUT_CAPITAL = "ØªÙ…Ù„Ú© Ø¯Ø§Ø±Ø§ÛŒÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø§ÛŒ.xlsx"
INPUT_EXPENSE = "Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ù‡Ø²ÛŒÙ†Ù‡ Ø§ÛŒ.xlsx"
OUTPUT_FILE = "ÙØ§ÛŒÙ„_Ø¨Ø±Ø±Ø³ÛŒ_Ùˆ_Ø§ØµÙ„Ø§Ø­_Ø¯Ø³ØªÛŒ.xlsx"

# Minimum occurrences for clustering
MIN_CLUSTER_SIZE = 2

# Persian connector words - prefixes should NOT end with these
CONNECTOR_WORDS = {"Ùˆ", "Ø¯Ø±", "Ø¨Ù‡", "Ø§Ø²", "Ø¨Ø§", "Ø¨Ø±Ø§ÛŒ", "Ù‡Ø§ÛŒ", "Ø¬Ù‡Øª", "Ø±ÙˆÛŒ", "ØªØ§"}

# Words to remove from descriptions (noise)
NOISE_WORDS = ["Ù¾Ø±ÙˆÚ˜Ù‡", "Ø¹Ù…Ù„ÛŒØ§Øª", "Ø§Ø¬Ø±Ø§ÛŒ", "Ø§Ø¬Ø±Ø§", "Ø§Ù†Ø¬Ø§Ù…", "Ø·Ø±Ø­"]


# ============================================================
# CLEANING DICTIONARY (Layer 1)
# ============================================================

CLEANING_MAP = {
    # Urban & Services
    "Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ùˆ ØªÙˆØ³Ø¹Ù‡ ÙØ¶Ø§ÛŒ Ø³Ø¨Ø²": ["ÙØ¶Ø§ÛŒ Ø³Ø¨Ø²", "Ù¾Ø§Ø±Ú©", "Ø¯Ø±Ø®Øª", "Ú¯ÛŒØ§Ù‡", "Ø¢Ø¨ÛŒØ§Ø±ÛŒ", "Ú†Ù…Ù†", "Ø¨Ø§ØºØ¨Ø§Ù†ÛŒ"],
    "Ù†Ø¸Ø§ÙØª Ø´Ù‡Ø±ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø³Ù…Ø§Ù†Ø¯": ["Ù†Ø¸Ø§ÙØª", "Ø±ÙØª Ùˆ Ø±ÙˆØ¨", "Ø²Ø¨Ø§Ù„Ù‡", "Ù¾Ø³Ù…Ø§Ù†Ø¯", "Ø¬Ø§Ø±Ùˆ", "Ø¨Ø§Ø²ÛŒØ§ÙØª"],
    "Ù„Ø§ÛŒØ±ÙˆØ¨ÛŒ Ø§Ù†Ù‡Ø§Ø± Ùˆ Ù…Ø³ÛŒÙ„â€ŒÙ‡Ø§": ["Ù„Ø§ÛŒØ±ÙˆØ¨ÛŒ", "Ù…Ø§Ø¯ÛŒ", "Ù†Ù‡Ø±", "Ú©Ø§Ù†Ø§Ù„", "Ù…Ø³ÛŒÙ„"],
    "Ø¢Ø¨Ø±Ø³Ø§Ù†ÛŒ Ùˆ ØªØ§Ø³ÛŒØ³Ø§Øª Ø¢Ø¨ÛŒ": ["Ø¢Ø¨Ø±Ø³Ø§Ù†ÛŒ", "ØªØ§Ù†Ú©Ø±", "Ú†Ø§Ù‡", "Ù‚Ù†Ø§Øª", "Ù…Ù†Ø¨Ø¹ Ø¢Ø¨", "Ø´ÛŒØ± Ø¢ØªØ´ Ù†Ø´Ø§Ù†ÛŒ"],
    "ØªØ§Ø³ÛŒØ³Ø§Øª Ùˆ Ù…Ø¨Ù„Ù…Ø§Ù† Ø´Ù‡Ø±ÛŒ": ["Ù…Ø¨Ù„Ù…Ø§Ù†", "Ù†ÛŒÙ…Ú©Øª", "Ø³Ø·Ù„", "ØªØ§Ø³ÛŒØ³Ø§Øª Ø´Ù‡Ø±ÛŒ", "Ø¢Ø°ÛŒÙ† Ø¨Ù†Ø¯ÛŒ"],
    
    # Civil & Traffic
    "Ø±ÙˆÚ©Ø´ Ùˆ ØªØ±Ù…ÛŒÙ… Ø¢Ø³ÙØ§Ù„Øª": ["Ø¢Ø³ÙØ§Ù„Øª", "Ø±ÙˆÚ©Ø´", "Ù„Ú©Ù‡ Ú¯ÛŒØ±ÛŒ", "Ù‚ÛŒØ±"],
    "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆØ³Ø§Ø²ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ù…Ø¹Ø§Ø¨Ø±": ["Ù¾ÛŒØ§Ø¯Ù‡ Ø±Ùˆ", "Ø³Ù†Ú¯ ÙØ±Ø´", "Ø¨Ù„ÙˆÚ©", "Ú©Ù ÙØ±Ø´", "Ø²ÛŒØ±Ø³Ø§Ø²ÛŒ Ù…Ø¹Ø§Ø¨Ø±"],
    "Ø¬Ø¯ÙˆÙ„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ùˆ Ú©Ø§Ù†ÛŒÙˆ": ["Ø¬Ø¯ÙˆÙ„", "Ú©Ø§Ù†ÛŒÙˆ", "Ø¢Ø¨Ø±Ø§Ù‡Ù‡", "Ø¬Ø¯ÙˆÙ„Ú¯Ø°Ø§Ø±ÛŒ"],
    "ØªØ¬Ù‡ÛŒØ²Ø§Øª Ùˆ Ø¹Ù„Ø§Ø¦Ù… ØªØ±Ø§ÙÛŒÚ©ÛŒ": ["ØªØ±Ø§ÙÛŒÚ©", "Ø®Ø· Ú©Ø´ÛŒ", "ØªØ§Ø¨Ù„Ùˆ", "Ø³Ø±Ø¹Øª Ú¯ÛŒØ±", "Ú¯Ø§Ø±Ø¯Ø±ÛŒÙ„", "Ú†Ø±Ø§Øº Ø±Ø§Ù‡Ù†Ù…Ø§"],
    "Ù¾Ù„ Ùˆ ØªÙ‚Ø§Ø·Ø¹ ØºÛŒØ±Ù‡Ù…Ø³Ø·Ø­": ["Ù¾Ù„", "Ø²ÛŒØ±Ú¯Ø°Ø±", "Ø±ÙˆÚ¯Ø°Ø±", "ØªÙ‚Ø§Ø·Ø¹"],
    
    # Buildings & Facilities
    "Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ùˆ Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ": ["Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ", "Ù†ÙˆØ±Ù¾Ø±Ø¯Ø§Ø²ÛŒ", "Ø¨Ø±Ù‚", "Ù„ÙˆØ³ØªØ±", "Ú†Ø±Ø§Øº"],
    "ØªØ¹Ù…ÛŒØ± Ùˆ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø³Ø§Ø®ØªÙ…Ø§Ù†â€ŒÙ‡Ø§": ["Ø³Ø§Ø®ØªÙ…Ø§Ù†", "Ø§Ø¨Ù†ÛŒÙ‡", "ØªØ§Ø³ÛŒØ³Ø§Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†", "Ù…ÙˆØªÙˆØ±Ø®Ø§Ù†Ù‡", "ØªØ¹Ù…ÛŒØ±Ø§Øª"],
    "Ø§Ø­Ø¯Ø§Ø« Ùˆ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø§Ø¨Ù†ÛŒÙ‡": ["Ø§Ø­Ø¯Ø§Ø«", "Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ", "Ø³Ø§Ø®Øª", "ØªÚ©Ù…ÛŒÙ„"],
    "Ù…Ø±Ù…Øª Ùˆ Ù†ÙˆØ³Ø§Ø²ÛŒ": ["Ù…Ø±Ù…Øª", "Ù†ÙˆØ³Ø§Ø²ÛŒ", "Ø¨Ù‡Ø³Ø§Ø²ÛŒ", "Ø¨Ø§Ø²Ø¢ÙØ±ÛŒÙ†ÛŒ"],
    
    # Admin & HR
    "Ù¾Ø±Ø¯Ø§Ø®Øª Ø­Ù‚ÙˆÙ‚ Ùˆ Ù…Ø²Ø§ÛŒØ§": ["Ø­Ù‚ÙˆÙ‚", "Ø¯Ø³ØªÙ…Ø²Ø¯", "Ù…Ø²Ø§ÛŒØ§", "Ú©Ø§Ø±Ø§Ù†Ù‡", "ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡"],
    "Ø®Ø¯Ù…Ø§Øª Ø±ÙØ§Ù‡ÛŒ Ùˆ Ø§Ù†Ú¯ÛŒØ²Ø´ÛŒ": ["Ø±ÙØ§Ù‡ÛŒ", "Ù¾Ø§Ø¯Ø§Ø´", "Ø¨Ù†", "ÙˆØ±Ø²Ø´ÛŒ", "Ù‡Ø¯ÛŒÙ‡", "Ø¨ÛŒÙ…Ù‡ ØªÚ©Ù…ÛŒÙ„ÛŒ"],
    "Ø®Ø±ÛŒØ¯ Ù…Ù„Ø²ÙˆÙ…Ø§Øª Ùˆ ØªØ¬Ù‡ÛŒØ²Ø§Øª": ["Ø®Ø±ÛŒØ¯", "ØªØ¬Ù‡ÛŒØ²Ø§Øª", "Ù…Ù„Ø²ÙˆÙ…Ø§Øª", "Ø§Ø«Ø§Ø«ÛŒÙ‡", "Ù„ÙˆØ§Ø²Ù…"],
    "Ø®Ø¯Ù…Ø§Øª Ú†Ø§Ù¾ Ùˆ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª": ["Ú†Ø§Ù¾", "Ù†Ø´Ø±ÛŒØ§Øª", "Ø¨Ù†Ø±", "Ø§Ù†ØªØ´Ø§Ø±Ø§Øª"],
    
    # Financial & Legal
    "Ù¾Ø±Ø¯Ø§Ø®Øª Ø¯ÛŒÙˆÙ† Ùˆ ØªØ¹Ù‡Ø¯Ø§Øª": ["Ø¯ÛŒÙˆÙ†", "Ø§Ù†ØªÙ‚Ø§Ù„ ÙˆØ¬ÙˆÙ‡", "Ø¨Ø§Ø²Ù¾Ø±Ø¯Ø§Ø®Øª", "Ø¨Ø¯Ù‡ÛŒ"],
    "ØªÙ…Ù„Ú© Ùˆ Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø§Ø±Ø§Ø¶ÛŒ": ["ØªÙ…Ù„Ú©", "Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ", "Ù…Ø³ÛŒØ± Ú¯Ø´Ø§ÛŒÛŒ", "Ø¹Ø±ØµÙ‡", "Ø²Ù…ÛŒÙ†"],
    "Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø±Ú©ØªÛŒ": ["Ù…Ø´Ø§Ø±Ú©Øª", "Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ", "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ"],
    
    # Budget & Revenue
    "Ù…Ø¯ÛŒØ±ÛŒØª Ø¨ÙˆØ¯Ø¬Ù‡ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª": ["Ø¨ÙˆØ¯Ø¬Ù‡", "ØªØ®ØµÛŒØµ", "Ù…ÙˆØ§ÙÙ‚ØªÙ†Ø§Ù…Ù‡", "ØªÙØ±ÛŒØº", "Ø§Ø¹ØªØ¨Ø§Ø±"],
    "ÙˆØµÙˆÙ„ Ø¯Ø±Ø¢Ù…Ø¯ Ùˆ Ø¹ÙˆØ§Ø±Ø¶": ["Ø¹ÙˆØ§Ø±Ø¶", "Ù†ÙˆØ³Ø§Ø²ÛŒ", "Ú©Ø³Ø¨ Ùˆ Ù¾ÛŒØ´Ù‡", "Ø¯Ø±Ø¢Ù…Ø¯", "ÙˆØµÙˆÙ„"],
    "Ø§ØµÙÙ‡Ø§Ù† Ú©Ø§Ø±Øª": ["Ø§ØµÙÙ‡Ø§Ù† Ú©Ø§Ø±Øª", "Ø§ØµÙÙ‡Ø§Ù†â€ŒÚ©Ø§Ø±Øª", "Ú©Ø§Ø±Øª Ø´Ù‡Ø±ÙˆÙ†Ø¯ÛŒ"],
}


# ============================================================
# TRUSTEE -> SUBSYSTEM MAPPING
# ============================================================

SUBSYSTEM_NAMES = {
    "URBAN_PLANNING": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø´Ù‡Ø±Ø³Ø§Ø²ÛŒ",
    "CONTRACTS": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§Ù…ÙˆØ± Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§",
    "PAYROLL": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø­Ù‚ÙˆÙ‚ Ùˆ Ø¯Ø³ØªÙ…Ø²Ø¯",
    "TADAROKAT": "Ø³Ø§Ù…Ø§Ù†Ù‡ ØªØ¯Ø§Ø±Ú©Ø§Øª",
    "BUDGET": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¨ÙˆØ¯Ø¬Ù‡",
    "TREASURY": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø®Ø²Ø§Ù†Ù‡â€ŒØ¯Ø§Ø±ÛŒ",
    "CONTRACTORS": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§Ù…ÙˆØ± Ù¾ÛŒÙ…Ø§Ù†Ú©Ø§Ø±Ø§Ù†",
    "WELFARE": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø±ÙØ§Ù‡ Ú©Ø§Ø±Ú©Ù†Ø§Ù†",
    "REAL_ESTATE": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§Ù…Ù„Ø§Ú©",
    "WAREHOUSE": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§Ù†Ø¨Ø§Ø± Ùˆ Ø§Ù…ÙˆØ§Ù„",
    "REVENUE": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¯Ø±Ø¢Ù…Ø¯",
    "ISFAHAN_CARD": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§ØµÙÙ‡Ø§Ù† Ú©Ø§Ø±Øª",
    "INVESTMENT": "Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§",
    "OTHER": "Ø³Ø§ÛŒØ± / Ø¹Ù…ÙˆÙ…ÛŒ",
}

TRUSTEE_TO_SUBSYSTEM = {
    "Ù…Ø¹Ø§ÙˆÙ†Øª Ø®Ø¯Ù…Ø§Øª": "CONTRACTORS",
    "Ø®Ø¯Ù…Ø§Øª Ø´Ù‡Ø±ÛŒ": "CONTRACTORS",
    "Ù…Ø¹Ø§ÙˆÙ†Øª ÙÙ†ÛŒ": "CONTRACTORS",
    "ÙÙ†ÛŒ Ø¹Ù…Ø±Ø§Ù†ÛŒ": "CONTRACTORS",
    "Ø´Ù‡Ø±Ø³Ø§Ø²ÛŒ": "URBAN_PLANNING",
    "Ù…Ø¹Ù…Ø§Ø±ÛŒ": "URBAN_PLANNING",
    "Ù…Ø¹Ø§ÙˆÙ†Øª Ù…Ø§Ù„ÛŒ": "TREASURY",
    "Ù…Ø§Ù„ÛŒ": "TREASURY",
    "Ø®Ø²Ø§Ù†Ù‡": "TREASURY",
    "Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÛŒØ²ÛŒ": "BUDGET",
    "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ": "BUDGET",
    "Ø§Ù…ÙˆØ± Ø§Ø¯Ø§Ø±ÛŒ": "WAREHOUSE",
    "Ø¯Ø±Ø¢Ù…Ø¯": "REVENUE",
    "Ù…Ø´Ø§Ø±Ú©Øª": "INVESTMENT",
    "Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ": "INVESTMENT",
}


# ============================================================
# UTILITY FUNCTIONS
# ============================================================

def clean_text(text) -> str:
    """Clean and normalize Persian text."""
    if pd.isna(text) or text is None:
        return ""
    text = str(text).strip()
    text = text.replace("ÙŠ", "ÛŒ").replace("Ùƒ", "Ú©")
    return text


def find_column(df: pd.DataFrame, keywords: List[str]) -> Optional[str]:
    """Find a column containing any of the keywords."""
    for col in df.columns:
        col_str = str(col).strip()
        for kw in keywords:
            if kw in col_str:
                return col
    return None


def contains_any(text: str, keywords: List[str]) -> bool:
    """Check if text contains any of the keywords."""
    if not text:
        return False
    return any(kw in text for kw in keywords)


def remove_noise(text: str) -> str:
    """Remove noise words from text."""
    for word in NOISE_WORDS:
        text = text.replace(word, "")
    # Clean up extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_ngrams(text: str, n: int) -> Optional[str]:
    """Extract first N words from text."""
    # Remove numbers and special chars
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    text = re.sub(r'[ØŒ,\-_:Ø›]', ' ', text)
    
    words = text.split()
    if len(words) >= n:
        return ' '.join(words[:n])
    return None


def is_valid_prefix(prefix: str) -> bool:
    """Check if prefix is valid (doesn't end with connector word)."""
    if not prefix:
        return False
    words = prefix.split()
    if not words:
        return False
    last_word = words[-1]
    # Invalid if ends with connector
    if last_word in CONNECTOR_WORDS:
        return False
    # Invalid if too short
    if len(prefix) < 5:
        return False
    return True


def extend_prefix_if_needed(prefix: str, text: str) -> str:
    """
    If prefix ends with a connector word, extend it with the next word from text.
    """
    words_prefix = prefix.split()
    if not words_prefix:
        return prefix
    
    last_word = words_prefix[-1]
    if last_word not in CONNECTOR_WORDS:
        return prefix
    
    # Find the next word in the original text
    words_text = text.split()
    prefix_len = len(words_prefix)
    
    if prefix_len < len(words_text):
        # Add next word
        extended = prefix + ' ' + words_text[prefix_len]
        return extended
    
    return prefix


def classify_to_subsystem(trustee: str, subject: str, description: str, is_capital: bool) -> str:
    """Classify row to subsystem."""
    for pattern, subsystem in TRUSTEE_TO_SUBSYSTEM.items():
        if pattern in trustee:
            return subsystem
    
    # Keywords
    if "Ø­Ù‚ÙˆÙ‚" in subject or "Ø¯Ø³ØªÙ…Ø²Ø¯" in subject:
        return "PAYROLL"
    if "Ø±ÙØ§Ù‡ÛŒ" in description or "Ù¾Ø§Ø¯Ø§Ø´" in description:
        return "WELFARE"
    if "ØªÙ…Ù„Ú©" in description or "Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ" in description:
        return "REAL_ESTATE"
    if "Ù…Ø´Ø§Ø±Ú©Øª" in description:
        return "INVESTMENT"
    if "Ø¹ÙˆØ§Ø±Ø¶" in description or "Ø¯Ø±Ø¢Ù…Ø¯" in description:
        return "REVENUE"
    if "Ø¨ÙˆØ¯Ø¬Ù‡" in description or "Ø§Ø¹ØªØ¨Ø§Ø±" in description:
        return "BUDGET"
    
    return "CONTRACTORS" if is_capital else "OTHER"


# ============================================================
# SMART CLUSTERING (V7)
# ============================================================

def smart_cluster_descriptions(descriptions: List[str]) -> Dict[str, str]:
    """
    V7 Improved Clustering with Longest Match Priority.
    Returns: {raw_description: suggested_title}
    """
    # Step 1: Generate all n-grams (2, 3, 4 words) for all descriptions
    ngram_counts = defaultdict(int)
    desc_to_ngrams = defaultdict(list)
    
    for desc in descriptions:
        cleaned = remove_noise(desc)
        for n in [4, 3, 2]:  # Try longer first
            ngram = extract_ngrams(cleaned, n)
            if ngram and len(ngram) >= 5:
                # Extend if ends with connector
                ngram = extend_prefix_if_needed(ngram, cleaned)
                if is_valid_prefix(ngram):
                    ngram_counts[ngram] += 1
                    desc_to_ngrams[desc].append((n, ngram))
    
    # Step 2: Find valid clusters (n-grams appearing >= MIN_CLUSTER_SIZE times)
    valid_clusters = {ng for ng, count in ngram_counts.items() if count >= MIN_CLUSTER_SIZE}
    
    # Step 3: Assign each description to the LONGEST matching n-gram
    result = {}
    for desc, ngrams in desc_to_ngrams.items():
        # Sort by length (longer first)
        ngrams.sort(key=lambda x: x[0], reverse=True)
        
        matched = False
        for _, ngram in ngrams:
            if ngram in valid_clusters:
                result[desc] = ngram
                matched = True
                break
        
        if not matched:
            # No cluster found - use raw description for manual review
            result[desc] = desc
    
    # Handle descriptions with no ngrams
    for desc in descriptions:
        if desc not in result:
            result[desc] = desc
    
    return result


# ============================================================
# DATA LOADING
# ============================================================

def load_excel(filepath: str, budget_type: str, filter_continuous: bool = False) -> List[dict]:
    """Load Excel file and extract rows."""
    try:
        df = pd.read_excel(filepath, engine='openpyxl')
        print(f"   âœ… Loaded: {filepath} ({len(df):,} rows)")
    except Exception as e:
        print(f"   âŒ Error: {e}")
        return []
    
    # Find columns
    desc_col = find_column(df, ['Ø´Ø±Ø­ Ø±Ø¯ÛŒÙ', 'Ø´Ø±Ø­'])
    trustee_col = find_column(df, ['Ù…ØªÙˆÙ„ÛŒ', 'Ù…ØªÙˆÙ„ÙŠ'])
    subject_col = find_column(df, ['Ù…ÙˆØ¶ÙˆØ¹', 'Ø²ÛŒØ± Ù…ÙˆØ¶ÙˆØ¹'])
    row_type_col = find_column(df, ['Ù†ÙˆØ¹ Ø±Ø¯ÛŒÙ'])
    
    if not desc_col:
        print(f"   âš ï¸  No description column found!")
        return []
    
    # Filter for continuous if needed
    if filter_continuous and row_type_col:
        df = df[df[row_type_col].astype(str).str.contains('Ù…Ø³ØªÙ…Ø±', na=False)]
        print(f"   ğŸ”„ Filtered to 'Ù…Ø³ØªÙ…Ø±': {len(df):,} rows")
    
    rows = []
    for _, row in df.iterrows():
        desc = clean_text(row.get(desc_col, ''))
        if not desc:
            continue
        
        rows.append({
            'description': desc,
            'trustee': clean_text(row.get(trustee_col, '')) if trustee_col else '',
            'subject': clean_text(row.get(subject_col, '')) if subject_col else '',
            'budget_type': budget_type
        })
    
    return rows


# ============================================================
# MAIN PROCESSING
# ============================================================

def process_all_rows(all_rows: List[dict]) -> pd.DataFrame:
    """Process all rows through the V7 pipeline."""
    
    # Group by unique description
    desc_counter = Counter([r['description'] for r in all_rows])
    
    # Build lookup for budget type and subsystem
    desc_to_info = {}
    for row in all_rows:
        desc = row['description']
        if desc not in desc_to_info:
            is_capital = row['budget_type'] == 'capital'
            subsystem = classify_to_subsystem(
                row['trustee'], row['subject'], desc, is_capital
            )
            desc_to_info[desc] = {
                'subsystem': subsystem,
                'budget_type': row['budget_type']
            }
    
    # Layer 1: Dictionary matching
    dict_matched = {}
    unmatched_descs = []
    
    for desc in desc_counter.keys():
        matched = False
        for clean_title, keywords in CLEANING_MAP.items():
            if contains_any(desc, keywords):
                dict_matched[desc] = clean_title
                matched = True
                break
        if not matched:
            unmatched_descs.append(desc)
    
    print(f"\nğŸ“Š Layer 1 (Dictionary): {len(dict_matched)} matches")
    print(f"   Remaining for clustering: {len(unmatched_descs)}")
    
    # Layer 2: Smart Clustering (V7)
    clustered = smart_cluster_descriptions(unmatched_descs)
    
    cluster_count = sum(1 for d, c in clustered.items() if d != c)
    raw_count = sum(1 for d, c in clustered.items() if d == c)
    print(f"ğŸ“Š Layer 2 (Clustering): {cluster_count} clustered")
    print(f"ğŸ“Š Layer 3 (Raw/Manual): {raw_count} for review")
    
    # Build output rows
    output_rows = []
    
    for desc, count in desc_counter.items():
        info = desc_to_info.get(desc, {'subsystem': 'OTHER', 'budget_type': 'unknown'})
        
        # Determine suggested title
        if desc in dict_matched:
            suggested = dict_matched[desc]
        elif desc in clustered:
            suggested = clustered[desc]
        else:
            suggested = desc
        
        # Translate budget type
        budget_type_fa = "Ø¹Ù…Ø±Ø§Ù†ÛŒ (Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒØ§ÛŒ)" if info['budget_type'] == 'capital' else "Ø¬Ø§Ø±ÛŒ (Ù‡Ø²ÛŒÙ†Ù‡â€ŒØ§ÛŒ)"
        
        output_rows.append({
            'Ù†Ø§Ù… Ø³Ø§Ù…Ø§Ù†Ù‡': SUBSYSTEM_NAMES.get(info['subsystem'], 'Ø³Ø§ÛŒØ±'),
            'Ø´Ø±Ø­ Ø±Ø¯ÛŒÙ Ø§ØµÙ„ÛŒ': desc,
            'Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ': suggested,
            'Ù†ÙˆØ¹ Ø¨ÙˆØ¯Ø¬Ù‡': budget_type_fa,
            'ØªÚ©Ø±Ø§Ø±': count
        })
    
    # Sort by subsystem and then by count
    df = pd.DataFrame(output_rows)
    df = df.sort_values(['Ù†Ø§Ù… Ø³Ø§Ù…Ø§Ù†Ù‡', 'ØªÚ©Ø±Ø§Ø±'], ascending=[True, False])
    
    return df


def style_excel(filepath: str):
    """Apply styling to Excel file."""
    wb = load_workbook(filepath)
    ws = wb.active
    
    # Styles
    header_font = Font(bold=True, size=11, color="FFFFFF")
    header_fill = PatternFill(start_color="2F5496", end_color="2F5496", fill_type="solid")
    header_align = Alignment(horizontal="center", vertical="center", wrap_text=True)
    
    cell_align = Alignment(horizontal="right", vertical="center", wrap_text=True)
    editable_fill = PatternFill(start_color="FFFACD", end_color="FFFACD", fill_type="solid")  # Light yellow
    
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    # Style header
    for cell in ws[1]:
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = header_align
        cell.border = thin_border
    
    # Style data rows
    for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row), start=2):
        for col_idx, cell in enumerate(row, start=1):
            cell.alignment = cell_align
            cell.border = thin_border
            # Highlight editable column (Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ = column 3)
            if col_idx == 3:
                cell.fill = editable_fill
    
    # Column widths
    column_widths = {
        'A': 28,  # Ù†Ø§Ù… Ø³Ø§Ù…Ø§Ù†Ù‡
        'B': 60,  # Ø´Ø±Ø­ Ø±Ø¯ÛŒÙ Ø§ØµÙ„ÛŒ
        'C': 45,  # Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ (EDITABLE)
        'D': 20,  # Ù†ÙˆØ¹ Ø¨ÙˆØ¯Ø¬Ù‡
        'E': 10,  # ØªÚ©Ø±Ø§Ø±
    }
    
    for col, width in column_widths.items():
        ws.column_dimensions[col].width = width
    
    # Row heights
    ws.row_dimensions[1].height = 30
    for row_idx in range(2, ws.max_row + 1):
        ws.row_dimensions[row_idx].height = 25
    
    # Freeze header and RTL
    ws.freeze_panes = "A2"
    ws.sheet_view.rightToLeft = True
    
    wb.save(filepath)


# ============================================================
# MAIN
# ============================================================

def main():
    print("=" * 70)
    print("ğŸ“Š CONFIG GENERATOR V7 - REVIEWABLE EXCEL")
    print("=" * 70)
    print()
    print("V7 Improvements:")
    print("  â€¢ Longest Match Priority (4-word prefixes before 2-word)")
    print("  â€¢ Connector Awareness (extends 'Ù…Ø±Ù…Øª Ùˆ' to 'Ù…Ø±Ù…Øª Ùˆ Ù†ÙˆØ³Ø§Ø²ÛŒ')")
    print("  â€¢ Raw Fallback for Manual Review (no 'Ø³Ø§ÛŒØ±' hiding)")
    print()
    
    # Load data
    print("ğŸ“ Loading Excel files...")
    capital_rows = load_excel(INPUT_CAPITAL, 'capital', filter_continuous=True)
    expense_rows = load_excel(INPUT_EXPENSE, 'expense', filter_continuous=False)
    
    all_rows = capital_rows + expense_rows
    print(f"\n   Total rows: {len(all_rows):,}")
    
    # Process
    print("\nğŸ”„ Processing through V7 Pipeline...")
    df = process_all_rows(all_rows)
    
    # Save
    print(f"\nğŸ’¾ Saving to: {OUTPUT_FILE}")
    df.to_excel(OUTPUT_FILE, index=False, engine='openpyxl')
    
    # Style
    print("ğŸ¨ Applying styling...")
    style_excel(OUTPUT_FILE)
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“‹ SUMMARY")
    print("=" * 70)
    print(f"   Output: {OUTPUT_FILE}")
    print(f"   Unique Descriptions: {len(df)}")
    print()
    print("   ğŸ“ INSTRUCTIONS FOR REVIEWER:")
    print("   1. Open the Excel file")
    print("   2. Review the 'Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ' column (highlighted in YELLOW)")
    print("   3. Edit titles that are incorrect or too long")
    print("   4. Save the file for import into the final config")
    print("=" * 70)


if __name__ == "__main__":
    main()
