"""
Master Config Generator V6 - Common Prefix Clustering
=======================================================
Generates app/config/config_master.json using a THREE-LAYER cleaning pipeline:

Layer 1: Golden Dictionary (CLEANING_MAP) - Highest Priority
Layer 2: Common Prefix Clustering - Groups similar descriptions
Layer 3: Strict Fallback - "ÿ≥ÿß€åÿ± ÿÆÿØŸÖÿßÿ™ [Subsystem]"

This eliminates garbage data (specific names like streets) while preserving
meaningful activity categories.

Usage:
    python scripts/generate_config_v6_clustering.py
"""

import pandas as pd
import json
import re
from collections import defaultdict, Counter
from typing import Optional, List, Dict, Set, Tuple
from datetime import datetime


# ============================================================
# CONFIGURATION
# ============================================================

INPUT_CAPITAL = "ÿ™ŸÖŸÑ⁄© ÿØÿßÿ±ÿß€å€å ÿ≥ÿ±ŸÖÿß€åŸá ÿß€å.xlsx"
INPUT_EXPENSE = "ÿßÿπÿ™ÿ®ÿßÿ±ÿßÿ™ Ÿáÿ≤€åŸÜŸá ÿß€å.xlsx"
OUTPUT_FILE = "app/config/config_master.json"

# Minimum occurrences for a prefix to become a cluster
MIN_CLUSTER_SIZE = 3

# Minimum prefix length (words) for clustering
MIN_PREFIX_WORDS = 2


# ============================================================
# THE 13 SUBSYSTEMS DEFINITION
# ============================================================

SUBSYSTEMS = {
    "URBAN_PLANNING": {
        "code": "URBAN_PLANNING",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿ¥Ÿáÿ±ÿ≥ÿßÿ≤€å",
        "persian_name": "ÿ¥Ÿáÿ±ÿ≥ÿßÿ≤€å",
        "icon": "Building2",
        "attachment_type": "both",
        "order": 1
    },
    "CONTRACTS": {
        "code": "CONTRACTS",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿßŸÖŸàÿ± ŸÇÿ±ÿßÿ±ÿØÿßÿØŸáÿß",
        "persian_name": "ŸÇÿ±ÿßÿ±ÿØÿßÿØŸáÿß",
        "icon": "FileText",
        "attachment_type": "both",
        "order": 2
    },
    "PAYROLL": {
        "code": "PAYROLL",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿ≠ŸÇŸàŸÇ Ÿà ÿØÿ≥ÿ™ŸÖÿ≤ÿØ",
        "persian_name": "ÿ≠ŸÇŸàŸÇ Ÿà ÿØÿ≥ÿ™ŸÖÿ≤ÿØ",
        "icon": "Users",
        "attachment_type": "api",
        "order": 3
    },
    "TADAROKAT": {
        "code": "TADAROKAT",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿ™ÿØÿßÿ±⁄©ÿßÿ™",
        "persian_name": "ÿ™ÿØÿßÿ±⁄©ÿßÿ™",
        "icon": "ShoppingCart",
        "attachment_type": "upload",
        "order": 4
    },
    "BUDGET": {
        "code": "BUDGET",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿ®ŸàÿØÿ¨Ÿá",
        "persian_name": "ÿ®ŸàÿØÿ¨Ÿá",
        "icon": "BarChart3",
        "attachment_type": "none",
        "order": 5
    },
    "TREASURY": {
        "code": "TREASURY",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿÆÿ≤ÿßŸÜŸá‚ÄåÿØÿßÿ±€å",
        "persian_name": "ÿÆÿ≤ÿßŸÜŸá‚ÄåÿØÿßÿ±€å",
        "icon": "Vault",
        "attachment_type": "upload",
        "order": 6
    },
    "CONTRACTORS": {
        "code": "CONTRACTORS",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿßŸÖŸàÿ± Ÿæ€åŸÖÿßŸÜ⁄©ÿßÿ±ÿßŸÜ",
        "persian_name": "Ÿæ€åŸÖÿßŸÜ⁄©ÿßÿ±ÿßŸÜ",
        "icon": "HardHat",
        "attachment_type": "both",
        "order": 7
    },
    "WELFARE": {
        "code": "WELFARE",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿ±ŸÅÿßŸá ⁄©ÿßÿ±⁄©ŸÜÿßŸÜ",
        "persian_name": "ÿ±ŸÅÿßŸá",
        "icon": "Heart",
        "attachment_type": "upload",
        "order": 8
    },
    "REAL_ESTATE": {
        "code": "REAL_ESTATE",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿßŸÖŸÑÿß⁄©",
        "persian_name": "ÿßŸÖŸÑÿß⁄©",
        "icon": "Home",
        "attachment_type": "both",
        "order": 9
    },
    "WAREHOUSE": {
        "code": "WAREHOUSE",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿßŸÜÿ®ÿßÿ± Ÿà ÿßŸÖŸàÿßŸÑ",
        "persian_name": "ÿßŸÜÿ®ÿßÿ± Ÿà ÿßŸÖŸàÿßŸÑ",
        "icon": "Package",
        "attachment_type": "upload",
        "order": 10
    },
    "REVENUE": {
        "code": "REVENUE",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿØÿ±ÿ¢ŸÖÿØ",
        "persian_name": "ÿØÿ±ÿ¢ŸÖÿØ",
        "icon": "TrendingUp",
        "attachment_type": "api",
        "order": 11
    },
    "ISFAHAN_CARD": {
        "code": "ISFAHAN_CARD",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ÿßÿµŸÅŸáÿßŸÜ ⁄©ÿßÿ±ÿ™",
        "persian_name": "ÿßÿµŸÅŸáÿßŸÜ ⁄©ÿßÿ±ÿ™",
        "icon": "CreditCard",
        "attachment_type": "api",
        "order": 12
    },
    "INVESTMENT": {
        "code": "INVESTMENT",
        "title": "ÿ≥ÿßŸÖÿßŸÜŸá ŸÖÿ¥ÿßÿ±⁄©ÿ™‚ÄåŸáÿß Ÿà ÿ≥ÿ±ŸÖÿß€åŸá‚Äå⁄Øÿ∞ÿßÿ±€å",
        "persian_name": "ŸÖÿ¥ÿßÿ±⁄©ÿ™‚ÄåŸáÿß",
        "icon": "Handshake",
        "attachment_type": "both",
        "order": 13
    },
    "OTHER": {
        "code": "OTHER",
        "title": "ÿ≥ÿß€åÿ± / ÿπŸÖŸàŸÖ€å",
        "persian_name": "ÿ≥ÿß€åÿ±",
        "icon": "MoreHorizontal",
        "attachment_type": "upload",
        "order": 14
    }
}


# ============================================================
# LAYER 1: GOLDEN DICTIONARY (CLEANING_MAP)
# ============================================================

CLEANING_MAP = {
    # Urban & Services
    "ŸÜ⁄ØŸáÿØÿßÿ±€å Ÿà ÿ™Ÿàÿ≥ÿπŸá ŸÅÿ∂ÿß€å ÿ≥ÿ®ÿ≤": ["ŸÅÿ∂ÿß€å ÿ≥ÿ®ÿ≤", "Ÿæÿßÿ±⁄©", "ÿØÿ±ÿÆÿ™", "⁄Ø€åÿßŸá", "ÿ¢ÿ®€åÿßÿ±€å", "⁄ÜŸÖŸÜ", "ÿ®ÿßÿ∫ÿ®ÿßŸÜ€å"],
    "ŸÜÿ∏ÿßŸÅÿ™ ÿ¥Ÿáÿ±€å Ÿà ŸÖÿØ€åÿ±€åÿ™ Ÿæÿ≥ŸÖÿßŸÜÿØ": ["ŸÜÿ∏ÿßŸÅÿ™", "ÿ±ŸÅÿ™ Ÿà ÿ±Ÿàÿ®", "ÿ≤ÿ®ÿßŸÑŸá", "Ÿæÿ≥ŸÖÿßŸÜÿØ", "ÿ¨ÿßÿ±Ÿà", "ÿ®ÿßÿ≤€åÿßŸÅÿ™"],
    "ŸÑÿß€åÿ±Ÿàÿ®€å ÿßŸÜŸáÿßÿ± Ÿà ŸÖÿ≥€åŸÑ‚ÄåŸáÿß": ["ŸÑÿß€åÿ±Ÿàÿ®€å", "ŸÖÿßÿØ€å", "ŸÜŸáÿ±", "⁄©ÿßŸÜÿßŸÑ", "ŸÖÿ≥€åŸÑ"],
    "ÿ¢ÿ®ÿ±ÿ≥ÿßŸÜ€å Ÿà ÿ™ÿßÿ≥€åÿ≥ÿßÿ™ ÿ¢ÿ®€å": ["ÿ¢ÿ®ÿ±ÿ≥ÿßŸÜ€å", "ÿ™ÿßŸÜ⁄©ÿ±", "⁄ÜÿßŸá", "ŸÇŸÜÿßÿ™", "ŸÖŸÜÿ®ÿπ ÿ¢ÿ®"],
    "ÿ™ÿßÿ≥€åÿ≥ÿßÿ™ Ÿà ŸÖÿ®ŸÑŸÖÿßŸÜ ÿ¥Ÿáÿ±€å": ["ŸÖÿ®ŸÑŸÖÿßŸÜ", "ŸÜ€åŸÖ⁄©ÿ™", "ÿ≥ÿ∑ŸÑ", "ÿ™ÿßÿ≥€åÿ≥ÿßÿ™ ÿ¥Ÿáÿ±€å", "ÿ¢ÿ∞€åŸÜ ÿ®ŸÜÿØ€å"],
    
    # Civil & Traffic
    "ÿ±Ÿà⁄©ÿ¥ Ÿà ÿ™ÿ±ŸÖ€åŸÖ ÿ¢ÿ≥ŸÅÿßŸÑÿ™": ["ÿ¢ÿ≥ŸÅÿßŸÑÿ™", "ÿ±Ÿà⁄©ÿ¥", "ŸÑ⁄©Ÿá ⁄Ø€åÿ±€å", "ŸÇ€åÿ±"],
    "Ÿæ€åÿßÿØŸá‚Äåÿ±Ÿàÿ≥ÿßÿ≤€å Ÿà ÿßÿµŸÑÿßÿ≠ ŸÖÿπÿßÿ®ÿ±": ["Ÿæ€åÿßÿØŸá ÿ±Ÿà", "ÿ≥ŸÜ⁄Ø ŸÅÿ±ÿ¥", "ÿ®ŸÑŸà⁄©", "⁄©ŸÅ ŸÅÿ±ÿ¥", "ÿ≤€åÿ±ÿ≥ÿßÿ≤€å ŸÖÿπÿßÿ®ÿ±"],
    "ÿ¨ÿØŸàŸÑ‚Äå⁄Øÿ∞ÿßÿ±€å Ÿà ⁄©ÿßŸÜ€åŸà": ["ÿ¨ÿØŸàŸÑ", "⁄©ÿßŸÜ€åŸà", "ÿ¢ÿ®ÿ±ÿßŸáŸá", "ÿ¨ÿØŸàŸÑ⁄Øÿ∞ÿßÿ±€å"],
    "ÿ™ÿ¨Ÿá€åÿ≤ÿßÿ™ Ÿà ÿπŸÑÿßÿ¶ŸÖ ÿ™ÿ±ÿßŸÅ€å⁄©€å": ["ÿ™ÿ±ÿßŸÅ€å⁄©", "ÿÆÿ∑ ⁄©ÿ¥€å", "ÿ™ÿßÿ®ŸÑŸà", "ÿ≥ÿ±ÿπÿ™ ⁄Ø€åÿ±", "⁄Øÿßÿ±ÿØÿ±€åŸÑ", "⁄Üÿ±ÿßÿ∫ ÿ±ÿßŸáŸÜŸÖÿß"],
    "ŸæŸÑ Ÿà ÿ™ŸÇÿßÿ∑ÿπ ÿ∫€åÿ±ŸáŸÖÿ≥ÿ∑ÿ≠": ["ŸæŸÑ", "ÿ≤€åÿ±⁄Øÿ∞ÿ±", "ÿ±Ÿà⁄Øÿ∞ÿ±", "ÿ™ŸÇÿßÿ∑ÿπ"],
    
    # Buildings & Facilities
    "ÿ±Ÿàÿ¥ŸÜÿß€å€å Ÿà ŸÜŸàÿ±Ÿæÿ±ÿØÿßÿ≤€å": ["ÿ±Ÿàÿ¥ŸÜÿß€å€å", "ŸÜŸàÿ±Ÿæÿ±ÿØÿßÿ≤€å", "ÿ®ÿ±ŸÇ", "ŸÑŸàÿ≥ÿ™ÿ±", "⁄Üÿ±ÿßÿ∫"],
    "ÿ™ÿπŸÖ€åÿ± Ÿà ŸÜ⁄ØŸáÿØÿßÿ±€å ÿ≥ÿßÿÆÿ™ŸÖÿßŸÜ‚ÄåŸáÿß": ["ÿ≥ÿßÿÆÿ™ŸÖÿßŸÜ", "ÿßÿ®ŸÜ€åŸá", "ÿ™ÿßÿ≥€åÿ≥ÿßÿ™ ÿ≥ÿßÿÆÿ™ŸÖÿßŸÜ", "ŸÖŸàÿ™Ÿàÿ±ÿÆÿßŸÜŸá", "ÿ™ÿπŸÖ€åÿ±ÿßÿ™"],
    "ÿßÿ≠ÿØÿßÿ´ Ÿà ÿ®ÿßÿ≤ÿ≥ÿßÿ≤€å ÿßÿ®ŸÜ€åŸá": ["ÿßÿ≠ÿØÿßÿ´", "ÿ®ÿßÿ≤ÿ≥ÿßÿ≤€å", "ÿ≥ÿßÿÆÿ™", "ÿ™⁄©ŸÖ€åŸÑ"],
    
    # Admin & HR
    "Ÿæÿ±ÿØÿßÿÆÿ™ ÿ≠ŸÇŸàŸÇ Ÿà ŸÖÿ≤ÿß€åÿß": ["ÿ≠ŸÇŸàŸÇ", "ÿØÿ≥ÿ™ŸÖÿ≤ÿØ", "ŸÖÿ≤ÿß€åÿß", "⁄©ÿßÿ±ÿßŸÜŸá", "ŸÅŸàŸÇ ÿßŸÑÿπÿßÿØŸá"],
    "ÿÆÿØŸÖÿßÿ™ ÿ±ŸÅÿßŸá€å Ÿà ÿßŸÜ⁄Ø€åÿ≤ÿ¥€å": ["ÿ±ŸÅÿßŸá€å", "ŸæÿßÿØÿßÿ¥", "ÿ®ŸÜ", "Ÿàÿ±ÿ≤ÿ¥€å", "ŸáÿØ€åŸá", "ÿ®€åŸÖŸá ÿ™⁄©ŸÖ€åŸÑ€å"],
    "ÿÆÿ±€åÿØ ŸÖŸÑÿ≤ŸàŸÖÿßÿ™ Ÿà ÿ™ÿ¨Ÿá€åÿ≤ÿßÿ™": ["ÿÆÿ±€åÿØ", "ÿ™ÿ¨Ÿá€åÿ≤ÿßÿ™", "ŸÖŸÑÿ≤ŸàŸÖÿßÿ™", "ÿßÿ´ÿßÿ´€åŸá", "ŸÑŸàÿßÿ≤ŸÖ"],
    "ÿÆÿØŸÖÿßÿ™ ⁄ÜÿßŸæ Ÿà ÿßŸÜÿ™ÿ¥ÿßÿ±ÿßÿ™": ["⁄ÜÿßŸæ", "ŸÜÿ¥ÿ±€åÿßÿ™", "ÿ®ŸÜÿ±", "ÿßŸÜÿ™ÿ¥ÿßÿ±ÿßÿ™"],
    
    # Financial & Legal
    "Ÿæÿ±ÿØÿßÿÆÿ™ ÿØ€åŸàŸÜ Ÿà ÿ™ÿπŸáÿØÿßÿ™": ["ÿØ€åŸàŸÜ", "ÿßŸÜÿ™ŸÇÿßŸÑ Ÿàÿ¨ŸàŸá", "ÿ®ÿßÿ≤Ÿæÿ±ÿØÿßÿÆÿ™", "ÿ®ÿØŸá€å"],
    "ÿ™ŸÖŸÑ⁄© Ÿà ÿ¢ÿ≤ÿßÿØÿ≥ÿßÿ≤€å ÿßÿ±ÿßÿ∂€å": ["ÿ™ŸÖŸÑ⁄©", "ÿ¢ÿ≤ÿßÿØÿ≥ÿßÿ≤€å", "ŸÖÿ≥€åÿ±", "ÿπÿ±ÿµŸá", "ÿ≤ŸÖ€åŸÜ"],
    "Ÿæÿ±Ÿà⁄òŸá‚ÄåŸáÿß€å ŸÖÿ¥ÿßÿ±⁄©ÿ™€å": ["ŸÖÿ¥ÿßÿ±⁄©ÿ™", "ÿ≥ÿ±ŸÖÿß€åŸá ⁄Øÿ∞ÿßÿ±€å", "ÿ≥ÿ±ŸÖÿß€åŸá‚Äå⁄Øÿ∞ÿßÿ±€å"],
    
    # Budget & Revenue (New)
    "ŸÖÿØ€åÿ±€åÿ™ ÿ®ŸàÿØÿ¨Ÿá Ÿà ÿßÿπÿ™ÿ®ÿßÿ±ÿßÿ™": ["ÿ®ŸàÿØÿ¨Ÿá", "ÿ™ÿÆÿµ€åÿµ", "ŸÖŸàÿßŸÅŸÇÿ™ŸÜÿßŸÖŸá", "ÿ™ŸÅÿ±€åÿ∫", "ÿßÿπÿ™ÿ®ÿßÿ±"],
    "ŸàÿµŸàŸÑ ÿØÿ±ÿ¢ŸÖÿØ Ÿà ÿπŸàÿßÿ±ÿ∂": ["ÿπŸàÿßÿ±ÿ∂", "ŸÜŸàÿ≥ÿßÿ≤€å", "⁄©ÿ≥ÿ® Ÿà Ÿæ€åÿ¥Ÿá", "ÿØÿ±ÿ¢ŸÖÿØ", "ŸàÿµŸàŸÑ"],
    "ÿßÿµŸÅŸáÿßŸÜ ⁄©ÿßÿ±ÿ™": ["ÿßÿµŸÅŸáÿßŸÜ ⁄©ÿßÿ±ÿ™", "ÿßÿµŸÅŸáÿßŸÜ‚Äå⁄©ÿßÿ±ÿ™", "⁄©ÿßÿ±ÿ™ ÿ¥Ÿáÿ±ŸàŸÜÿØ€å"],
}


# ============================================================
# TRUSTEE -> SUBSYSTEM MAPPING
# ============================================================

TRUSTEE_TO_SUBSYSTEM = {
    "ŸÖÿπÿßŸàŸÜÿ™ ÿÆÿØŸÖÿßÿ™": "CONTRACTORS",
    "ÿÆÿØŸÖÿßÿ™ ÿ¥Ÿáÿ±€å": "CONTRACTORS",
    "ŸÖÿπÿßŸàŸÜÿ™ ÿÆÿØŸÖÿßÿ™ ÿ¥Ÿáÿ±€å": "CONTRACTORS",
    "ŸÖÿπÿßŸàŸÜÿ™ ŸÅŸÜ€å": "CONTRACTORS",
    "ŸÅŸÜ€å ÿπŸÖÿ±ÿßŸÜ€å": "CONTRACTORS",
    "ŸÖÿπÿßŸàŸÜÿ™ ŸÅŸÜ€å Ÿà ÿπŸÖÿ±ÿßŸÜ€å": "CONTRACTORS",
    "ÿ¥Ÿáÿ±ÿ≥ÿßÿ≤€å": "URBAN_PLANNING",
    "ŸÖÿπÿßŸàŸÜÿ™ ÿ¥Ÿáÿ±ÿ≥ÿßÿ≤€å": "URBAN_PLANNING",
    "ŸÖÿπŸÖÿßÿ±€å": "URBAN_PLANNING",
    "ŸÖÿπÿßŸàŸÜÿ™ ŸÖÿßŸÑ€å": "TREASURY",
    "ŸÖÿßŸÑ€å": "TREASURY",
    "ÿÆÿ≤ÿßŸÜŸá": "TREASURY",
    "ÿ®ÿ±ŸÜÿßŸÖŸá ÿ±€åÿ≤€å": "BUDGET",
    "ÿ®ÿ±ŸÜÿßŸÖŸá‚Äåÿ±€åÿ≤€å": "BUDGET",
    "ÿßŸÖŸàÿ± ÿßÿØÿßÿ±€å": "WAREHOUSE",
    "ÿßÿØÿßÿ±€å": "WAREHOUSE",
    "ÿØÿ±ÿ¢ŸÖÿØ": "REVENUE",
    "ŸÖÿ¥ÿßÿ±⁄©ÿ™": "INVESTMENT",
    "ÿ≥ÿ±ŸÖÿß€åŸá ⁄Øÿ∞ÿßÿ±€å": "INVESTMENT",
}


# ============================================================
# KEYWORD -> SUBSYSTEM MAPPING
# ============================================================

KEYWORD_TO_SUBSYSTEM = {
    "ISFAHAN_CARD": ["ÿßÿµŸÅŸáÿßŸÜ ⁄©ÿßÿ±ÿ™", "ÿßÿµŸÅŸáÿßŸÜ‚Äå⁄©ÿßÿ±ÿ™"],
    "WELFARE": ["ÿ±ŸÅÿßŸá€å", "ŸæÿßÿØÿßÿ¥", "Ÿàÿ±ÿ≤ÿ¥€å", "ÿ®ŸÜ ⁄©ÿßÿ±ÿ™", "ÿ®€åŸÖŸá ÿ™⁄©ŸÖ€åŸÑ€å", "ŸáÿØ€åŸá"],
    "PAYROLL": ["ÿ≠ŸÇŸàŸÇ", "ÿØÿ≥ÿ™ŸÖÿ≤ÿØ", "ŸÖÿ≤ÿß€åÿß", "⁄©ÿßÿ±ÿßŸÜŸá", "ŸÅŸàŸÇ ÿßŸÑÿπÿßÿØŸá"],
    "REAL_ESTATE": ["ÿ™ŸÖŸÑ⁄©", "ÿ¢ÿ≤ÿßÿØÿ≥ÿßÿ≤€å", "ŸÖÿ≥€åÿ± ⁄Øÿ¥ÿß€å€å", "ÿßÿ±ÿßÿ∂€å", "ŸÖŸÑ⁄©"],
    "TREASURY": ["ÿØ€åŸàŸÜ", "ÿßŸÜÿ™ŸÇÿßŸÑ Ÿàÿ¨ŸàŸá", "ÿ®ÿßŸÜ⁄©€å", "ÿÆÿ≤ÿßŸÜŸá", "⁄Ü⁄©", "ÿ≠ŸàÿßŸÑŸá"],
    "TADAROKAT": ["ÿÆÿ±€åÿØ", "ŸÖŸÑÿ≤ŸàŸÖÿßÿ™", "ÿ™ÿ¨Ÿá€åÿ≤ÿßÿ™", "⁄ÜÿßŸæ", "ŸÑŸàÿßÿ≤ŸÖ"],
    "INVESTMENT": ["ŸÖÿ¥ÿßÿ±⁄©ÿ™", "ÿ≥ÿ±ŸÖÿß€åŸá ⁄Øÿ∞ÿßÿ±€å"],
    "BUDGET": ["ÿ®ŸàÿØÿ¨Ÿá", "ÿ™ÿÆÿµ€åÿµ", "ŸÖŸàÿßŸÅŸÇÿ™ŸÜÿßŸÖŸá", "ÿßÿπÿ™ÿ®ÿßÿ±"],
    "REVENUE": ["ÿπŸàÿßÿ±ÿ∂", "ÿØÿ±ÿ¢ŸÖÿØ", "ŸàÿµŸàŸÑ", "ŸÜŸàÿ≥ÿßÿ≤€å"],
    "CONTRACTORS": ["ÿßÿ≠ÿØÿßÿ´", "ÿ™⁄©ŸÖ€åŸÑ", "ÿ≤€åÿ±ÿ≥ÿßÿ≤€å", "ÿ¢ÿ≥ŸÅÿßŸÑÿ™", "ÿ¨ÿØŸàŸÑ", "ÿ≥ÿßÿÆÿ™", "ÿπŸÖÿ±ÿßŸÜ€å",
                    "Ÿæ€åÿßÿØŸá ÿ±Ÿà", "ŸæŸÑ", "ÿ≤€åÿ±⁄Øÿ∞ÿ±", "ŸÅÿ∂ÿß€å ÿ≥ÿ®ÿ≤", "ŸÜÿ∏ÿßŸÅÿ™", "ŸÑÿß€åÿ±Ÿàÿ®€å"],
    "WAREHOUSE": ["ÿßŸÜÿ®ÿßÿ±", "ÿßŸÖŸàÿßŸÑ", "⁄©ÿßŸÑÿß"],
}


# ============================================================
# UTILITY FUNCTIONS
# ============================================================

def clean_text(text) -> str:
    """Clean and normalize Persian text."""
    if pd.isna(text) or text is None:
        return ""
    text = str(text).strip()
    # Normalize Arabic characters to Persian
    text = text.replace("Ÿä", "€å").replace("ŸÉ", "⁄©")
    return text


def find_column(df: pd.DataFrame, keywords: List[str]) -> Optional[str]:
    """Find a column containing any of the keywords."""
    for col in df.columns:
        col_str = str(col).strip()
        for kw in keywords:
            if kw in col_str:
                return col
    return None


def contains_any(text: str, keywords: List[str]) -> bool:
    """Check if text contains any of the keywords."""
    if not text:
        return False
    return any(kw in text for kw in keywords)


def extract_prefix(text: str, num_words: int = 2) -> str:
    """Extract the first N words from text as a prefix."""
    # Remove noise patterns
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\(.*?\)', '', text)  # Remove parentheses content
    text = re.sub(r'[ÿå,\-_:]', ' ', text)  # Replace punctuation with space
    
    words = text.split()
    if len(words) >= num_words:
        return ' '.join(words[:num_words])
    elif len(words) > 0:
        return ' '.join(words)
    return ""


def get_fallback_title(subsystem_code: str) -> str:
    """Generate fallback title for a subsystem."""
    persian_name = SUBSYSTEMS.get(subsystem_code, {}).get("persian_name", "ÿ≥ÿß€åÿ±")
    return f"ÿ≥ÿß€åÿ± ÿÆÿØŸÖÿßÿ™ {persian_name}"


# ============================================================
# LAYER 1: DICTIONARY MATCHING
# ============================================================

def match_dictionary(description: str) -> Optional[str]:
    """Layer 1: Check if description matches any keyword in CLEANING_MAP."""
    for clean_title, keywords in CLEANING_MAP.items():
        if contains_any(description, keywords):
            return clean_title
    return None


# ============================================================
# LAYER 2: COMMON PREFIX CLUSTERING
# ============================================================

def build_prefix_clusters(descriptions: List[str], min_size: int = 3) -> Dict[str, str]:
    """
    Build clusters from common prefixes.
    Returns a mapping: raw_description -> cluster_title
    """
    # Count prefix occurrences
    prefix_counter = Counter()
    desc_to_prefix = {}
    
    for desc in descriptions:
        prefix = extract_prefix(desc, num_words=MIN_PREFIX_WORDS)
        if prefix and len(prefix) >= 5:  # Minimum prefix length
            prefix_counter[prefix] += 1
            desc_to_prefix[desc] = prefix
    
    # Find valid clusters (prefixes appearing >= min_size times)
    valid_clusters = {prefix for prefix, count in prefix_counter.items() if count >= min_size}
    
    # Map descriptions to their cluster title
    cluster_mapping = {}
    for desc, prefix in desc_to_prefix.items():
        if prefix in valid_clusters:
            cluster_mapping[desc] = prefix
    
    return cluster_mapping


# ============================================================
# SUBSYSTEM CLASSIFICATION
# ============================================================

def classify_to_subsystem(row_data: dict, is_capital: bool) -> str:
    """Classify a row to a subsystem using waterfall logic."""
    trustee = row_data.get('trustee', '')
    subject = row_data.get('subject', '')
    description = row_data.get('description', '')
    
    # Level 1: Trustee Check
    for trustee_pattern, subsystem in TRUSTEE_TO_SUBSYSTEM.items():
        if trustee_pattern in trustee:
            return subsystem
    
    # Level 2: Subject Check (for Payroll)
    if contains_any(subject, KEYWORD_TO_SUBSYSTEM.get("PAYROLL", [])):
        return "PAYROLL"
    
    # Level 3: Description Keywords
    for subsystem, patterns in KEYWORD_TO_SUBSYSTEM.items():
        if subsystem == "CONTRACTORS" and not is_capital:
            continue
        if contains_any(description, patterns):
            return subsystem
    
    # Level 4: Fallback
    return "CONTRACTORS" if is_capital else "OTHER"


# ============================================================
# DATA LOADING
# ============================================================

def load_excel_with_budget_type(filepath: str, budget_type: str, 
                                 filter_continuous: bool = False) -> List[dict]:
    """Load Excel file and tag all rows with budget type."""
    try:
        df = pd.read_excel(filepath, engine='openpyxl')
        print(f"   ‚úÖ Loaded: {filepath} ({len(df):,} rows)")
    except Exception as e:
        print(f"   ‚ùå Error loading {filepath}: {e}")
        return []
    
    # Find columns
    desc_col = find_column(df, ['ÿ¥ÿ±ÿ≠ ÿ±ÿØ€åŸÅ', 'ÿ¥ÿ±ÿ≠'])
    trustee_col = find_column(df, ['ŸÖÿ™ŸàŸÑ€å', 'ŸÖÿ™ŸàŸÑŸä'])
    subject_col = find_column(df, ['ŸÖŸàÿ∂Ÿàÿπ', 'ÿ≤€åÿ± ŸÖŸàÿ∂Ÿàÿπ'])
    row_type_col = find_column(df, ['ŸÜŸàÿπ ÿ±ÿØ€åŸÅ'])
    
    if not desc_col:
        print(f"   ‚ö†Ô∏è  No description column found!")
        return []
    
    # Filter for continuous rows if needed
    if filter_continuous and row_type_col:
        df = df[df[row_type_col].astype(str).str.contains('ŸÖÿ≥ÿ™ŸÖÿ±', na=False)]
        print(f"   üîÑ Filtered to 'ŸÖÿ≥ÿ™ŸÖÿ±': {len(df):,} rows")
    
    # Process rows
    rows = []
    for _, row in df.iterrows():
        desc = clean_text(row.get(desc_col, ''))
        if not desc:
            continue
        
        rows.append({
            'description': desc,
            'trustee': clean_text(row.get(trustee_col, '')) if trustee_col else '',
            'subject': clean_text(row.get(subject_col, '')) if subject_col else '',
            'budget_type': budget_type
        })
    
    return rows


# ============================================================
# THREE-LAYER PROCESSING PIPELINE
# ============================================================

def process_with_three_layers(all_rows: List[dict]) -> Dict[str, Dict[str, Set[str]]]:
    """
    Process all rows through the three-layer pipeline.
    Returns: {subsystem: {budget_type: set(activity_titles)}}
    """
    # Group rows by subsystem first
    subsystem_rows = defaultdict(list)
    
    for row in all_rows:
        is_capital = row['budget_type'] == 'capital'
        subsystem = classify_to_subsystem(row, is_capital)
        subsystem_rows[subsystem].append(row)
    
    # Results: {subsystem: {budget_type: set(titles)}}
    results = defaultdict(lambda: defaultdict(set))
    
    # Stats
    stats = {'layer1': 0, 'layer2': 0, 'layer3': 0}
    
    for subsystem, rows in subsystem_rows.items():
        # Separate by budget type
        capital_rows = [r for r in rows if r['budget_type'] == 'capital']
        expense_rows = [r for r in rows if r['budget_type'] == 'expense']
        
        for budget_type, budget_rows in [('capital', capital_rows), ('expense', expense_rows)]:
            if not budget_rows:
                continue
            
            # Rows not matched by Layer 1
            unmatched_descriptions = []
            
            for row in budget_rows:
                desc = row['description']
                
                # LAYER 1: Dictionary Match
                dict_title = match_dictionary(desc)
                if dict_title:
                    results[subsystem][budget_type].add(dict_title)
                    stats['layer1'] += 1
                else:
                    unmatched_descriptions.append(desc)
            
            # LAYER 2: Common Prefix Clustering (on unmatched)
            if unmatched_descriptions:
                cluster_mapping = build_prefix_clusters(unmatched_descriptions, MIN_CLUSTER_SIZE)
                
                for desc in unmatched_descriptions:
                    if desc in cluster_mapping:
                        # Use cluster title (prefix)
                        results[subsystem][budget_type].add(cluster_mapping[desc])
                        stats['layer2'] += 1
                    else:
                        # LAYER 3: Strict Fallback
                        fallback = get_fallback_title(subsystem)
                        results[subsystem][budget_type].add(fallback)
                        stats['layer3'] += 1
    
    print(f"\nüìä Layer Statistics:")
    print(f"   Layer 1 (Dictionary): {stats['layer1']:,} matches")
    print(f"   Layer 2 (Clustering): {stats['layer2']:,} matches")
    print(f"   Layer 3 (Fallback):   {stats['layer3']:,} fallbacks")
    
    return results


# ============================================================
# JSON GENERATION
# ============================================================

def build_activity(subsystem_code: str, title: str, index: int, budget_type: str) -> dict:
    """Build activity JSON with proper budget type constraint."""
    return {
        "code": f"{subsystem_code}_{index:02d}",
        "title": title,
        "form_type": None,
        "frequency": "MONTHLY",
        "requires_file_upload": False,
        "external_service_url": None,
        "order": index,
        "is_active": True,
        "constraints": [
            {
                "budget_code_pattern": None,
                "allowed_budget_types": [budget_type],
                "cost_center_pattern": None,
                "allowed_cost_centers": None,
                "constraint_type": "INCLUDE",
                "priority": 1,
                "description": f"ŸÅŸÇÿ∑ ÿ±ÿØ€åŸÅ‚ÄåŸáÿß€å ÿ®ŸàÿØÿ¨Ÿá {'ÿ≥ÿ±ŸÖÿß€åŸá‚Äåÿß€å' if budget_type == 'capital' else 'Ÿáÿ≤€åŸÜŸá‚Äåÿß€å'}"
            }
        ]
    }


def build_subsystem(code: str, activities_by_budget: Dict[str, Set[str]]) -> dict:
    """Build subsystem JSON with activities."""
    subsystem_def = SUBSYSTEMS.get(code, SUBSYSTEMS["OTHER"])
    
    activities = []
    idx = 1
    seen = set()
    
    # Add expense activities first
    for title in sorted(activities_by_budget.get('expense', set())):
        if title not in seen:
            activities.append(build_activity(code, title, idx, 'expense'))
            seen.add(title)
            idx += 1
    
    # Add capital activities
    for title in sorted(activities_by_budget.get('capital', set())):
        if title not in seen:
            activities.append(build_activity(code, title, idx, 'capital'))
            seen.add(title)
            idx += 1
    
    return {
        "code": subsystem_def["code"],
        "title": subsystem_def["title"],
        "icon": subsystem_def["icon"],
        "attachment_type": subsystem_def["attachment_type"],
        "order": subsystem_def["order"],
        "is_active": len(activities) > 0,
        "activities": activities
    }


# ============================================================
# MAIN
# ============================================================

def main():
    print("=" * 70)
    print("üìä CONFIG GENERATOR V6 - COMMON PREFIX CLUSTERING")
    print("=" * 70)
    print()
    print("Three-Layer Pipeline:")
    print("  Layer 1: Golden Dictionary (CLEANING_MAP)")
    print("  Layer 2: Common Prefix Clustering (min 3 occurrences)")
    print("  Layer 3: Strict Fallback (ÿ≥ÿß€åÿ± ÿÆÿØŸÖÿßÿ™ [Subsystem])")
    print()
    
    # Load data with budget type tagging
    print("üìÅ Loading Excel files...")
    capital_rows = load_excel_with_budget_type(INPUT_CAPITAL, 'capital', filter_continuous=True)
    expense_rows = load_excel_with_budget_type(INPUT_EXPENSE, 'expense', filter_continuous=False)
    
    all_rows = capital_rows + expense_rows
    print(f"\n   Total rows to process: {len(all_rows):,}")
    
    # Process through three layers
    print("\nüîÑ Processing through Three-Layer Pipeline...")
    results = process_with_three_layers(all_rows)
    
    # Build JSON structure
    print("\nüî® Building JSON structure...")
    subsystems_json = []
    
    for code in sorted(SUBSYSTEMS.keys(), key=lambda x: SUBSYSTEMS[x]["order"]):
        activities_by_budget = results.get(code, {})
        if activities_by_budget:
            subsystems_json.append(build_subsystem(code, activities_by_budget))
    
    # Create final config
    config = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "version": "6.0.0",
        "generated_at": datetime.now().isoformat(),
        "description": "Config V6 - Three-layer cleaning with Common Prefix Clustering",
        "subsystems": subsystems_json
    }
    
    # Save to file
    import os
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # Summary
    total_activities = sum(len(s['activities']) for s in config['subsystems'])
    
    print("\n" + "=" * 70)
    print("üìã SUMMARY")
    print("=" * 70)
    print(f"   Output File: {OUTPUT_FILE}")
    print(f"   Total Subsystems: {len(config['subsystems'])}")
    print(f"   Total Activities: {total_activities}")
    print()
    print("   Per Subsystem:")
    for s in config['subsystems']:
        print(f"   ‚Ä¢ {s['title']}: {len(s['activities'])} activities")
    
    print("\n‚úÖ Config V6 generated successfully!")
    print("=" * 70)


if __name__ == "__main__":
    main()
